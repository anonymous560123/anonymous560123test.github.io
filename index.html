<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
    content="SSACD: Semantic Shift-Aware Contrastive Decoding for mitigating hallucinations and omissions in Video Large Language Models.">
  <meta name="keywords" content="SSACD, Video-LLM, Hallucination, Contrastive Decoding, Scene Transition">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SSACD: Semantic Shift-Aware Contrastive Decoding</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

    </div>
  </div>
</nav>

<!-- ===================== HERO ===================== -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SSACD</h1>
          <h2 class="subtitle is-4 publication-title">
            Semantic Shift-Aware Contrastive Decoding<br>
            for Mitigating Hallucinations and Omissions<br>
            in Video Large Language Models
          </h2>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Anonymous</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-6">
            <span class="author-block"><sup>1</sup>Affiliation withheld for double-blind review</span>
          </div>

          <div class="column has-text-centered" style="margin-top: 1.5rem;">
            <div class="publication-links">
              <!-- TODO: 실제 링크로 교체하세요 -->
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Optional teaser video -->
<!--
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        SSACD demonstration on scene transition hallucination benchmarks.
      </h2>
    </div>
  </div>
</section>
-->

<section class="section">
  <div class="container is-max-desktop">

    <!-- ============= Abstract ============= -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video Large Language Models (Video LLMs) have achieved strong performance on video understanding
            tasks such as captioning, summarization, and question answering, yet they still struggle to interpret
            <em>scene transitions</em> reliably. In particular, they frequently
            hallucinate non-existent transitions, overlook true ones, or misinterpret the semantic relationship
            between pre- and post-transition scenes. We identify a key culprit behind these errors:
            the <strong>narrative prior</strong> – the model’s tendency to enforce semantic–temporal coherence even when
            consecutive frames exhibit abrupt semantic shifts.
          </p>
          <p>
            We propose <strong>SSACD (Semantic Shift-Aware Contrastive Decoding)</strong>, a training-free decoding
            method designed to mitigate such narrative-prior-driven hallucinations and omissions. SSACD detects
            semantic shifts using a CLIP-based encoder, injects Gaussian noise into frames after the most
            prominent shift to deliberately amplify semantic differences, and then contrasts logit distributions
            from the original and perturbed videos to suppress prior-induced bias during decoding.
          </p>
          <p>
            Extensive experiments on scene-transition benchmarks demonstrate that SSACD consistently improves
            performance without additional training. On VidHalluc, SSACD boosts scene transition understanding
            by up to <strong>40.3%</strong> for LLaVA-OneVision and <strong>14.5%</strong> for Qwen2.5-VL; on MVBench,
            it yields gains of <strong>32.6%</strong> and <strong>5.8%</strong>, respectively, while preserving general
            video understanding performance on Video-MME, TempCompass, and MMBench-Video.
          </p>
        </div>
      </div>
    </div>

    <!-- ============= Problem: Scene Transition Misunderstanding ============= -->
    <div class="columns is-centered has-text-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Scene Transition Misunderstanding</h2>
        <div class="has-text-centered" style="margin: 2rem 0;">
          <!-- Fig.1: 세 가지 에러 타입 예시 -->
          <img src="./static/images/1_errorcase.png"
               alt="Examples of hallucination, omission, and misinterpretation in scene transitions"
               style="max-width: 100%; height: auto;">
        </div>
        <div class="content has-text-justified">
          <p>
            We focus on <strong>scene transition misunderstanding</strong>, a reasoning-level failure mode in which
            Video LLMs mis-handle visual and semantic changes between consecutive scenes. This manifests as:
          </p>
          <ul>
            <li><strong>Hallucination</strong>: falsely detecting a scene change that does not actually occur.</li>
            <li><strong>Omission</strong>: overlooking a real scene transition and treating distinct scenes as continuous.</li>
            <li><strong>Misinterpretation</strong>: distorting the “from–to” relationship between scenes, often by inserting
                imaginary intermediate events to maintain narrative coherence.</li>
          </ul>
          <p>
            In instructional videos, procedural narrative priors bias the model to expect a new step whenever
            it believes the previous step has finished, encouraging hallucinated transitions. In storytelling videos,
            global narrative priors lead the model to merge or rewrite events to maintain a single coherent storyline,
            resulting in omissions and misinterpretations.
          </p>
        </div>
      </div>
    </div>

    <!-- ============= Method: SSACD ============= -->
    <div class="columns is-centered has-text-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">SSACD: Semantic Shift-Aware Contrastive Decoding</h2>
        <div class="has-text-centered" style="margin: 2rem 0;">
          <!-- Fig.2: SSACD 파이프라인 -->
          <img src="./static/images/2_method.png"
               alt="Overview of the SSACD pipeline"
               style="max-width: 100%; height: auto;">
        </div>
        <div class="content has-text-justified">
          <p>
            SSACD is a <strong>training-free contrastive decoding framework</strong> that operates purely at inference
            time. Given a video and an instruction, we construct two branches:
            the original video and a <em>semantic-shift–aware perturbed video</em>.
          </p>
          <ol>
            <li>
              <strong>Semantic Shift Detection.</strong>
              We apply a CLIP (ViT-B/32) visual encoder to each sampled frame and compute cosine distances
              between adjacent frames. Frames whose distance exceeds a threshold are treated as semantic shift
              positions. The index with the maximum shift is selected as the primary candidate for a scene boundary.
            </li>
            <li>
              <strong>Targeted Perturbation.</strong>
              To amplify narrative-prior-driven misunderstanding, we inject Gaussian noise into all frames
              <em>after</em> the maximum semantic shift index. This sharpens the semantic boundary between
              pre- and post-shift segments, encouraging the model to rely more heavily on its inductive biases
              in the perturbed branch.
            </li>
            <li>
              <strong>Contrastive Decoding.</strong>
              At each decoding step, the Video LLM produces token-level logits for both original and perturbed
              videos. SSACD combines them using a contrastive rule:
              <code>f̂ = (1 + α) f(v) − α f(v')</code>, optionally restricted to tokens whose base probability exceeds
              a plausibility threshold β. Tokens strongly preferred in the perturbed branch but unsupported by
              the original input are down-weighted, suppressing hallucination- and omission-inducing biases.
            </li>
          </ol>
          <p>
            Importantly, SSACD requires <strong>no additional training or finetuning</strong> and can be applied to a wide
            range of pretrained Video LLM backbones as a drop-in decoding strategy.
          </p>
        </div>
      </div>
    </div>

    <!-- ============= Benchmarks & Setup ============= -->
    <div class="columns is-centered has-text-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Benchmarks &amp; Experimental Setup</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate SSACD on three scene-transition benchmarks:
          </p>
          <ul>
            <li>
              <strong>VidHalluc (STH)</strong> – evaluates scene transition hallucinations via
              <em>Classification</em> (binary transition detection), <em>Description</em> (accuracy of pre/post location descriptions),
              and a composite <em>Overall</em> score.
            </li>
            <li>
              <strong>MVBench (ST)</strong> – a four-way multiple-choice task that tests whether the model
              can select the correct scene transition among plausible distractors.
            </li>
            <li>
              <strong>NOAH</strong> – a narrative prior–oriented benchmark where inserted clips create explicit scene transitions;
              we use the Existence QA task and the Inserted Event Omission Rate (IEOR) for captioning.
            </li>
          </ul>
          <p>
            To verify that SSACD does not harm general video understanding, we also evaluate on:
            <strong>Video-MME</strong>, <strong>TempCompass</strong>, and <strong>MMBench-Video</strong>, using lightweight splits for efficiency.
          </p>
          <p>
            Unless otherwise specified, we use <strong>Qwen2.5-VL (7B)</strong> and <strong>LLaVA-OneVision (7B)</strong> as backbones.
            Eight frames per video are uniformly sampled for CLIP-based semantic shift detection, and we perform
            a grid search over contrastive weights (α, β) for each benchmark, averaging results over three decoding seeds.
          </p>
        </div>
      </div>
    </div>

    <!-- ============= Main Results ============= -->
    <div class="columns is-centered has-text-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          <h3 class="title is-5 has-text-centered" style="color: #666; margin-top: 1rem;">Scene Transition Benchmarks</h3>
          <p>
            SSACD achieves consistent gains across VidHalluc, MVBench, and NOAH, outperforming
            prior training-free methods such as DINO-HEAL, VCD, and TCD.
          </p>
          <div class="has-text-centered" style="margin: 1.5rem 0;">
            <!-- Table 1: main 결과 -->
            <img src="./static/images/3_mainresult.png"
                 alt="Main results on VidHalluc, MVBench, and NOAH"
                 style="max-width: 100%; height: auto;">
          </div>
          <p>
            On VidHalluc (STH), SSACD reduces hallucination, omission, and misinterpretation error rates,
            leading to large improvements in the Overall score for both Qwen2.5-VL and LLaVA-OneVision.
            On MVBench, SSACD substantially improves accuracy on the scene transition subtask,
            indicating stronger robustness against plausible but incorrect transition candidates.
            On NOAH, SSACD lowers IEOR (fewer inserted events are ignored) and improves Existence classification,
            meaning the model is more likely to detect and describe inserted clips rather than smoothing them away.
          </p>

          <h3 class="title is-5 has-text-centered" style="color: #666; margin-top: 2.5rem;">General Video Understanding</h3>
          <p>
            To ensure that SSACD does not trade off general reasoning for better scene transitions,
            we evaluate on Video-MME, TempCompass, and MMBench-Video.
          </p>
          <div class="has-text-centered" style="margin: 1.5rem 0;">
            <!-- Table 2: general benchmark -->
            <img src="./static/images/4_general.png"
                 alt="Performance on general video understanding benchmarks"
                 style="max-width: 100%; height: auto;">
          </div>
          <p>
            Across all backbones and benchmarks, SSACD matches or slightly improves baseline scores within normal seed variance,
            indicating that it <strong>specifically targets scene transition misunderstanding</strong> while preserving
            overall video understanding capability.
          </p>
        </div>
      </div>
    </div>

    <!-- ============= Analysis ============= -->
    <div class="columns is-centered has-text-centered" style="margin-top: 3rem;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Analysis</h2>

        <!-- Semantic shift amplification -->
        <h3 class="title is-5 has-text-centered" style="color: #666; margin-top: 1.5rem;">Semantic Shift Amplification</h3>
        <div class="has-text-centered" style="margin: 1.5rem 0;">
          <!-- Fig.3: semantic shift bar plot -->
          <img src="./static/images/5_semanticshift.png"
               alt="Semantic shift amplification at transition points"
               style="max-width: 100%; height: auto;">
        </div>
        <div class="content has-text-justified">
          <p>
            By comparing CLIP-based semantic distances between adjacent frames before and after perturbation,
            we show that SSACD sharpens the semantic boundary at the detected transition frame.
            Perturbed-only variants (without contrastive decoding) indeed increase hallucination and misinterpretation
            rates, confirming that amplified semantic shifts strengthen narrative-prior-driven errors –
            a desirable property for constructing strong contrastive signals.
          </p>
        </div>

        <!-- Case studies -->
        <h3 class="title is-5 has-text-centered" style="color: #666; margin-top: 2.5rem;">Narrative Prior in Case Studies</h3>
        <div class="has-text-centered" style="margin: 1.5rem 0;">
          <!-- Fig.4: case study 예시 -->
          <img src="./static/images/6_casestudy.png"
               alt="Case studies of hallucination, omission, and misinterpretation corrected by SSACD"
               style="max-width: 100%; height: auto;">
        </div>
        <div class="content has-text-justified">
          <p>
            Qualitative examples from VidHalluc reveal how narrative priors shape the model’s predictions:
            in instructional videos, baseline models hallucinate new steps whenever the visual focus changes;
            in storytelling videos, they either merge semantically similar scenes or invent intermediate events
            to maintain global coherence. SSACD encourages the model to explicitly acknowledge abrupt transitions
            and avoid fabricating unseen content, leading to more faithful scene-level descriptions.
          </p>
        </div>

        <!-- Perturbation ablation -->
        <h3 class="title is-5 has-text-centered" style="color: #666; margin-top: 2.5rem;">Perturbation Strategies</h3>
        <div class="has-text-centered" style="margin: 1.5rem 0;">
          <!-- Table 4: ablation -->
          <img src="./static/images/7_strategy.png"
               alt="Comparison of perturbation strategies for contrastive decoding"
               style="max-width: 100%; height: auto;">
        </div>
        <div class="content has-text-justified">
          <p>
            We compare several perturbation variants, including temporal downsampling, Gaussian noise on all frames,
            and semantic-shift–aware variants. While global perturbations yield moderate gains, they struggle when precise
            boundary localization is required. The proposed <strong>Gaussian-After-Full</strong> strategy in SSACD,
            which injects noise only after the maximum semantic shift, consistently achieves the best performance:
            it produces a strong, localized contrastive signal aligned with scene transitions while preserving
            clean evidence elsewhere in the video.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<!-- ============= BibTeX ============= -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{anonymous2026ssacd,
  author    = {Anonymous},
  title     = {SSACD: Semantic Shift-Aware Contrastive Decoding for Mitigating
               Hallucinations and Omissions in Video Large Language Models},
  journal   = {Under Review},
  year      = {2026},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">

    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a
            <a rel="license"
               href="http://creativecommons.org/licenses/by-sa/4.0/">
              Creative Commons Attribution-ShareAlike 4.0 International License
            </a>.
          </p>
          <p>
            This page reuses the
            <a href="https://github.com/nerfies/nerfies.github.io">template from Nerfies</a>.
            If you borrow this source code, please link back to the original template in the footer.
            Remember to remove any analytics code you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>